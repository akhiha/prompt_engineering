{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad055b6",
   "metadata": {},
   "source": [
    "# Prompt Chaining Examples\n",
    "\n",
    "This notebook collects four practical examples of chaining multiple prompts or steps together to accomplish a complex task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119fe76b",
   "metadata": {},
   "source": [
    "## Example 1: Research-Paper Digest → Quiz Builder\n",
    "This chain summarises a long paper, generates MCQs for its contributions, and verifies coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99906508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "summary_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=PromptTemplate.from_template('\\n{paper}\\n—\\nExtract a crisp abstract (≤150 words) and list the 5 main contributions.\\n'),\n",
    "    output_key='summary'\n",
    ")\n",
    "\n",
    "mcq_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=PromptTemplate.from_template('\\nContributions:\\n{summary}\\n—\\nWrite one challenging MCQ for EACH contribution. Indicate the correct option.\\n'),\n",
    "    output_key='mcqs'\n",
    ")\n",
    "\n",
    "review_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=PromptTemplate.from_template('\\nMCQs produced:\\n{mcqs}\\n—\\nDo we have exactly 5 questions, one per contribution?\\n- If yes, reply “✅ ready”.\\n- If any contribution lacks a question, add it and then reply “✅ ready”.\\n'),\n",
    "    output_key='final_mcqs'\n",
    ")\n",
    "\n",
    "paper_chain = SequentialChain(\n",
    "    chains=[summary_chain, mcq_chain, review_chain],\n",
    "    input_variables=['paper'],\n",
    "    output_variables=['final_mcqs'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example usage (requires OPENAI_API_KEY)\n",
    "# paper_text = open('some_paper.txt').read()\n",
    "# result = paper_chain.run(paper=paper_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e592f7",
   "metadata": {},
   "source": [
    "## Example 2: Raw Marks → Performance Dashboard → Parent Emails\n",
    "This chain analyses marks, drafts a short dashboard snippet, and prepares bilingual emails for at-risk students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf1107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "df = pd.read_csv('ds_marks.csv')\n",
    "\n",
    "analyze_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=PromptTemplate.from_template(\"\\nYou are a data-analytics tutor.\\nGiven this JSON list of dicts [{name, roll, marks}] (max 50),\\nreturn JSON:\\n  {{ 'safe_count': int,\\n     'risk_count': int,\\n     'at_risk': [{{name, roll, marks}}] }}\\nStudents scoring <40 % are at_risk.\\n\\nDATA:\\n{data}\\n\"),\n",
    "    output_key='analysis'\n",
    ")\n",
    "\n",
    "email_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=PromptTemplate.from_template('\\nCreate a personalised email for EACH entry in {{at_risk_json}}.\\nFormat:\\n### <Roll> – <Name>\\nSubject: Performance Alert – DS\\nBody (80-120 words):\\n  • Explain shortfall\\n  • Invite to remedial on Friday 4 pm\\n  • Close with one Telugu sentence encouraging them\\nReturn all emails in plain text.\\n'),\n",
    "    output_key='emails'\n",
    ")\n",
    "\n",
    "hod_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=PromptTemplate.from_template(\"\\nSafe: {{safe}}; At-Risk: {{risk}}\\nWrite a ≤120-word summary for HoD, mentioning remedial plan.\\nAppend the full email block below a 'Drafts' heading.\\nEmails:\\n{{emails}}\\n\"),\n",
    "    output_key='hod_report'\n",
    ")\n",
    "\n",
    "marks_chain = SequentialChain(\n",
    "    chains=[analyze_chain, email_chain, hod_chain],\n",
    "    input_variables=['data'],\n",
    "    output_variables=['hod_report'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example usage (requires OPENAI_API_KEY)\n",
    "# report = marks_chain.run(data=df.to_dict(orient='records'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9e00c",
   "metadata": {},
   "source": [
    "## Example 3: Bug Report → Root-Cause Hypothesis → Patch Suggestion\n",
    "This chain extracts traces from logs, reasons about the root cause, proposes a patch, and checks if security-sensitive code paths are touched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "extract_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=PromptTemplate.from_template('\\n{log}\\n—\\nPull out stack traces and the last 20 lines before the crash.\\n'),\n",
    "    output_key='extract'\n",
    ")\n",
    "\n",
    "cause_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=PromptTemplate.from_template('\\nTrace + context:\\n{extract}\\n—\\nSuggest the most likely root cause in ≤50 words and list any missing information you’d need to confirm.\\n'),\n",
    "    output_key='cause'\n",
    ")\n",
    "\n",
    "patch_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=PromptTemplate.from_template('\\nHypothesis:\\n{cause}\\n—\\nPropose a code-level fix (language: Java, Spring Boot). Show only the diff.\\n'),\n",
    "    output_key='patch'\n",
    ")\n",
    "\n",
    "safety_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=PromptTemplate.from_template(\"\\nProposed diff:\\n{patch}\\n—\\nDoes this patch alter authentication or encryption code paths?\\n• If yes, flag 'SECURITY REVIEW NEEDED'.\\n• If no, flag 'OK'.\\n\"),\n",
    "    output_key='status'\n",
    ")\n",
    "\n",
    "bug_chain = SequentialChain(\n",
    "    chains=[extract_chain, cause_chain, patch_chain, safety_chain],\n",
    "    input_variables=['log'],\n",
    "    output_variables=['status'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example usage (requires OPENAI_API_KEY)\n",
    "# status = bug_chain.run(log=open('server.log').read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf4336",
   "metadata": {},
   "source": [
    "## Example 4: Lecture Notes → Learning Objectives → MCQs → QA Check\n",
    "This chain extracts learning objectives, generates MCQs, and repeats until each objective has a matching question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db752bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain, SequentialChainException\n",
    "\n",
    "with open('embedded_systems_unit3.txt') as f:\n",
    "    lecture_text = f.read()\n",
    "\n",
    "obj_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=PromptTemplate.from_template('\\nTEXT:\\n{para}\\n----\\nProduce 3–5 precise learning objectives in bullet form.\\n'),\n",
    "    output_key='objectives'\n",
    ")\n",
    "\n",
    "mcq_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=PromptTemplate.from_template(\"\\nObjectives:\\n{objectives}\\n----\\nFor EACH objective generate exactly one 4-option MCQ.\\nReturn JSON list: [{{'objective': str, 'mcq': str, 'answer': 'A/B/C/D', 'bloom': 'L1-L6'}}]\\n\"),\n",
    "    output_key='mcqs'\n",
    ")\n",
    "\n",
    "check_chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=PromptTemplate.from_template(\"\\nObjectives:\\n{objectives}\\n\\nMCQs:\\n{mcqs}\\n----\\nVerify: Is count(objectives)==count(mcqs) and each objective appears verbatim in one MCQ?\\nIf any mismatch, reply 'FIX' and list missing objectives; else reply 'PASS'.\\n\"),\n",
    "    output_key='status'\n",
    ")\n",
    "\n",
    "def run_until_pass(para: str, max_loops=3):\n",
    "    for _ in range(max_loops):\n",
    "        objectives = obj_chain.run(para=para)\n",
    "        mcqs = mcq_chain.run(objectives=objectives)\n",
    "        status = check_chain.run(objectives=objectives, mcqs=mcqs)\n",
    "        if 'PASS' in status:\n",
    "            return mcqs\n",
    "    raise SequentialChainException('QA never passed; try again.')\n",
    "\n",
    "# Example usage (requires OPENAI_API_KEY)\n",
    "# mcq_block = run_until_pass(lecture_text)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
